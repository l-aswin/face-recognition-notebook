{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e813bd2-4584-4478-b375-702054bda9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n"
     ]
    }
   ],
   "source": [
    "from simple_image_download import simple_image_download as simp\n",
    "\n",
    "# Create an instance of the Downloader class\n",
    "downloader = simp.Downloader()\n",
    "\n",
    "# Use the downloader object to perform operations\n",
    "# For example, to download images:\n",
    "downloader.download(keywords='Trump', limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9d0fb34-ea07-4782-8751-20afabf76162",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Input Path\n",
    "video_path = \".\\\\scan3.mp4\"\n",
    "\n",
    "# Output Path\n",
    "output_dir = \".\\\\training\\\\aswin\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0383b43-3934-439f-848d-85bd9bbeb36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total frames: 113\n",
      "frame_interval: 22\n",
      "\n",
      "\n",
      "DONE\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a VideoCapture object to read the input video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get the total number of frames in the video\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print('total frames:',total_frames)\n",
    "\n",
    "# Calculate the frame interval to capture for 150 images\n",
    "no_samples = 5\n",
    "\n",
    "if no_samples > total_frames :\n",
    "    print('video length is too short for requested sample size')\n",
    "else:\n",
    "    frame_interval = total_frames // no_samples # change this number according to your needs \n",
    "    print('frame_interval:',frame_interval)\n",
    "\n",
    "    # Set the initial frame counter to 0\n",
    "    frame_counter = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        # Read a frame from the video\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            break\n",
    "        #I want to sample first 2 minutes of 5 minutes with frame interval 3 frames\n",
    "        # Check if this is the frame to capture\n",
    "        if frame_counter % frame_interval == 0 and frame_counter // frame_interval < no_samples:\n",
    "            # Save the frame as a JPEG image\n",
    "            output_path = os.path.join(output_dir, f'{frame_counter//frame_interval+1 :02}.jpg')\n",
    "            cv2.imwrite(output_path, frame)\n",
    "        \n",
    "        # Increment the frame counter\n",
    "        frame_counter += 1\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    print(\"\\n\\nDONE\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7bc6d42a-f829-4015-b6ec-cf3d63887afe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:47:41.543199Z",
     "start_time": "2025-10-28T09:47:29.281746Z"
    }
   },
   "source": [
    "# Face Detection with MTCNN (Multi-Task Cascaded Convolutional Neural Network)\n",
    "from os import listdir\n",
    "from os.path import isdir\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "from numpy import savez_compressed\n",
    "from numpy import asarray\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "\n",
    "# extract a single face from a given photograph\n",
    "def extract_face(filename, required_size=(160, 160)):\n",
    "    # load image from file\n",
    "    image = Image.open(filename)\n",
    "    # convert to RGB, if needed\n",
    "    image = image.convert('RGB')\n",
    "    # convert to array\n",
    "    pixels = asarray(image)\n",
    "    # create the detector, using default weights\n",
    "    detector = MTCNN()\n",
    "    # detect faces in the image\n",
    "    results = detector.detect_faces(pixels)\n",
    "    # check if face was detected\n",
    "    if results:\n",
    "        # extract the bounding box from the first face\n",
    "        x1, y1, width, height = results[0]['box']\n",
    "        # bug fix\n",
    "        x1, y1 = abs(x1), abs(y1)\n",
    "        x2, y2 = x1 + width, y1 + height\n",
    "        # extract the face\n",
    "        face = pixels[y1:y2, x1:x2]\n",
    "        # resize pixels to the model size\n",
    "        image = Image.fromarray(face)\n",
    "        image = image.resize(required_size)\n",
    "        face_array = asarray(image)\n",
    "        return face_array\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# load images and extract faces for all images in a directory\n",
    "def load_faces(directory):\n",
    "    faces = []\n",
    "    # enumerate files\n",
    "    for filename in listdir(directory):\n",
    "        # path\n",
    "        path = directory + filename\n",
    "        # get face\n",
    "        face = extract_face(path)\n",
    "        # check if face was detected\n",
    "        if face is not None:\n",
    "            # store\n",
    "            faces.append(face)\n",
    "    return faces\n",
    "\n",
    "# load a dataset that contains one subdir for each class that in turn contains images\n",
    "def load_dataset(directory):\n",
    "    X, y = [], []\n",
    "    # enumerate folders, on per class\n",
    "    for subdir in listdir(directory):\n",
    "        # path\n",
    "        path = directory + subdir + '\\\\'\n",
    "        # skip any files that might be in the dir\n",
    "        if not isdir(path):\n",
    "            continue\n",
    "        # load all faces in the subdirectory\n",
    "        faces = load_faces(path)\n",
    "        # create labels\n",
    "        labels = [subdir for _ in range(len(faces))]\n",
    "        # summarize progress\n",
    "        print('>loaded %d examples for class: %s' % (len(faces), subdir))\n",
    "        # store\n",
    "        X.extend(faces)\n",
    "        y.extend(labels)\n",
    "    return asarray(X), asarray(y)\n",
    "\n",
    "# load train dataset\n",
    "trainX, trainy = load_dataset('.\\\\training\\\\')\n",
    "print(trainX.shape, trainy.shape)\n",
    "# save arrays to one file in compressed format\n",
    "savez_compressed('.\\\\face-detection.npz', trainX, trainy)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded 5 examples for class: anna\n",
      ">loaded 7 examples for class: aswin\n",
      ">loaded 5 examples for class: tiffany\n",
      ">loaded 5 examples for class: tom cruise\n",
      ">loaded 5 examples for class: vijay sethupathi\n",
      "(27, 160, 160, 3) (27,)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "eb38d5c2-6aed-40c3-9602-da8e90c22277",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:49:23.387481Z",
     "start_time": "2025-10-28T09:49:17.857742Z"
    }
   },
   "source": [
    "# Face Feature Extracion using FaceNet (calculating the face embedding)\n",
    "from numpy import load\n",
    "from numpy import expand_dims\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "from keras_facenet import FaceNet\n",
    "\n",
    "# get the face embedding for one face\n",
    "def get_embedding(model, face_pixels):\n",
    " # scale pixel values\n",
    " face_pixels = face_pixels.astype('float32')\n",
    " samples = expand_dims(face_pixels, axis=0)\n",
    " # make prediction to get embedding\n",
    " yhat = model.embeddings(samples)\n",
    " return yhat[0]\n",
    "\n",
    "# load the face dataset\n",
    "data = load('.\\\\face-detection.npz')\n",
    "trainX, trainy  = data['arr_0'], data['arr_1']\n",
    "print('Loaded: ', trainX.shape, trainy.shape, )\n",
    "# load the facenet model\n",
    "print('Loading Model')\n",
    "MyFaceNet = FaceNet()\n",
    "print('Loaded Model')\n",
    "# convert each face in the train set to an embedding\n",
    "newTrainX = list()\n",
    "for face_pixels in trainX:\n",
    " embedding = get_embedding(MyFaceNet, face_pixels)\n",
    " newTrainX.append(embedding)\n",
    "newTrainX = asarray(newTrainX)\n",
    "print(newTrainX.shape)\n",
    "# save arrays to one file in compressed format\n",
    "savez_compressed('.\\\\embeddings\\\\face_embeddings.npz', newTrainX, trainy)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:  (27, 160, 160, 3) (27,)\n",
      "Loading Model\n",
      "Loaded Model\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2s/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 63ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 65ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 62ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 61ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 75ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 67ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 59ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 70ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 61ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 60ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 57ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 66ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 68ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 67ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 55ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 74ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 60ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 57ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 61ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 69ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 58ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 65ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 58ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 61ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 61ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 58ms/step\n",
      "(27, 512)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "19be3a07-02c3-4e51-a9f0-c55f96356821",
   "metadata": {},
   "source": [
    "Face Identification program"
   ]
  },
  {
   "cell_type": "code",
   "id": "bca1611c-6644-457d-86a6-5e8c5cc9b371",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:50:53.978091Z",
     "start_time": "2025-10-28T09:50:53.821711Z"
    }
   },
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import Normalizer, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "8bebb77f-7538-45b8-b889-6baa255faafa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:50:57.276834Z",
     "start_time": "2025-10-28T09:50:55.645578Z"
    }
   },
   "source": [
    "# Load face detection model\n",
    "detector = MTCNN()\n",
    "\n",
    "# Load FaceNet model\n",
    "facenet_model = FaceNet()\n",
    "\n",
    "# Load face embeddings\n",
    "data = np.load('.\\\\embeddings\\\\face_embeddings.npz')\n",
    "trainX, trainy = data['arr_0'], data['arr_1']\n",
    "\n",
    "# Normalize input vectors\n",
    "in_encoder = Normalizer(norm='l2')\n",
    "trainX = in_encoder.transform(trainX)\n",
    "\n",
    "# Label encode targets\n",
    "out_encoder = LabelEncoder()\n",
    "out_encoder.fit(trainy)\n",
    "trainy = out_encoder.transform(trainy)\n",
    "\n",
    "# Define the classes\n",
    "class_names = out_encoder.classes_\n",
    "class_names = np.append(class_names, 'unknown')\n",
    "\n",
    "# Train SVM classifier\n",
    "model = SVC(kernel='linear', probability=True)\n",
    "model.fit(trainX, trainy)\n",
    "\n",
    "# Define function to extract face embeddings\n",
    "def extract_face_embeddings(image):\n",
    "    # Detect faces in the image\n",
    "    faces = detector.detect_faces(image)\n",
    "    if not faces:\n",
    "        return None\n",
    "    # Extract the first face only\n",
    "    x1, y1, width, height = faces[0]['box']\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    face = image[y1:y2, x1:x2]\n",
    "    # Resize face to the size required by facenet model\n",
    "    face = cv2.resize(face, (160, 160))\n",
    "    # Preprocess the face for facenet model\n",
    "    face = face.astype('float32')\n",
    "    face = np.expand_dims(face, axis=0)\n",
    "    # Generate embeddings using facenet model\n",
    "    embeddings = facenet_model.embeddings(face)\n",
    "    return embeddings[0]\n",
    "\n",
    "# Define function to identify the identity of an input image\n",
    "def identify_person(image):\n",
    "    # Extract face embeddings from input image\n",
    "    embeddings = extract_face_embeddings(image)\n",
    "    if embeddings is None:\n",
    "        return None, None\n",
    "    # Normalize embeddings\n",
    "    embeddings = in_encoder.transform([embeddings])\n",
    "    # Predict the identity and confidence using SVM classifier\n",
    "    prediction = model.predict(embeddings)\n",
    "    confidence = model.predict_proba(embeddings)[0][prediction] * 100\n",
    "    prediction = out_encoder.inverse_transform(prediction)\n",
    "    return prediction[0].item(), confidence\n",
    "\n",
    "# Define function to identify the identity and confidence of an input image\n",
    "def identify_person_with_unknown(image, threshold=0.9):\n",
    "    # Extract face embeddings from input image\n",
    "    embeddings = extract_face_embeddings(image)\n",
    "    if embeddings is None:\n",
    "        return None, None\n",
    "    # Normalize embeddings\n",
    "    embeddings = in_encoder.transform([embeddings])\n",
    "    # Predict the identity and confidence using SVM classifier\n",
    "    predictions = model.predict_proba(embeddings)[0]\n",
    "    max_idx = np.argmax(predictions)\n",
    "    if predictions[max_idx] >= threshold:\n",
    "        prediction = out_encoder.inverse_transform([max_idx])\n",
    "        confidence = predictions[max_idx] * 100\n",
    "        return prediction[0].item(), confidence\n",
    "    else:\n",
    "        return \"unknown\", None\n",
    "\n",
    "# Example usage\n",
    "image = cv2.imread('.\\\\test\\\\aswin\\\\s3_41.png')\n",
    "person, confidence = identify_person_with_unknown(image)\n",
    "if person is None:\n",
    "    print('No face detected in the input image!')\n",
    "elif person == \"unknown\":\n",
    "    text = \"Unknown person\"\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(text)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    # Display the predicted name and confidence probability\n",
    "    text = f'Predicted: {str(person)} ({confidence:.2f}%)'\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(text)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 81\u001B[39m\n\u001B[32m     79\u001B[39m \u001B[38;5;66;03m# Example usage\u001B[39;00m\n\u001B[32m     80\u001B[39m image = cv2.imread(\u001B[33m'\u001B[39m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[33mtest\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[33maswin\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[33ms3_41.png\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m81\u001B[39m person, confidence = \u001B[43midentify_person_with_unknown\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     82\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m person \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m     83\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m'\u001B[39m\u001B[33mNo face detected in the input image!\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 64\u001B[39m, in \u001B[36midentify_person_with_unknown\u001B[39m\u001B[34m(image, threshold)\u001B[39m\n\u001B[32m     62\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34midentify_person_with_unknown\u001B[39m(image, threshold=\u001B[32m0.9\u001B[39m):\n\u001B[32m     63\u001B[39m     \u001B[38;5;66;03m# Extract face embeddings from input image\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m64\u001B[39m     embeddings = \u001B[43mextract_face_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     65\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m embeddings \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m     66\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 31\u001B[39m, in \u001B[36mextract_face_embeddings\u001B[39m\u001B[34m(image)\u001B[39m\n\u001B[32m     29\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mextract_face_embeddings\u001B[39m(image):\n\u001B[32m     30\u001B[39m     \u001B[38;5;66;03m# Detect faces in the image\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m     faces = \u001B[43mdetector\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdetect_faces\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     32\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m faces:\n\u001B[32m     33\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\aswin-github\\venv enviroment\\cv-keras\\Lib\\site-packages\\mtcnn\\mtcnn.py:150\u001B[39m, in \u001B[36mMTCNN.detect_faces\u001B[39m\u001B[34m(self, image, fit_to_image, limit_boundaries_landmarks, box_format, output_type, postprocess, batch_stack_justification, **kwargs)\u001B[39m\n\u001B[32m    147\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m tf.device(\u001B[38;5;28mself\u001B[39m._device):\n\u001B[32m    148\u001B[39m     \u001B[38;5;66;03m# Load the images into memory and normalize them into a single tensor\u001B[39;00m\n\u001B[32m    149\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m150\u001B[39m         images_raw = \u001B[43mload_images_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    151\u001B[39m         images_normalized, images_oshapes, pad_param = standarize_batch(images_raw,\n\u001B[32m    152\u001B[39m                                                                         justification=batch_stack_justification,\n\u001B[32m    153\u001B[39m                                                                         normalize=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    155\u001B[39m         bboxes_batch = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\aswin-github\\venv enviroment\\cv-keras\\Lib\\site-packages\\mtcnn\\utils\\images.py:320\u001B[39m, in \u001B[36mload_images_batch\u001B[39m\u001B[34m(images, dtype, device)\u001B[39m\n\u001B[32m    306\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    307\u001B[39m \u001B[33;03mLoads a batch of images into memory. If the images are not already in tensor or NumPy array format, \u001B[39;00m\n\u001B[32m    308\u001B[39m \u001B[33;03mthey are loaded from their file paths.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    317\u001B[39m \u001B[33;03m    list of tf.Tensor: A list of TensorFlow tensors representing the raw images.\u001B[39;00m\n\u001B[32m    318\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    319\u001B[39m is_tensor = tf.is_tensor(images[\u001B[32m0\u001B[39m]) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(images[\u001B[32m0\u001B[39m], np.ndarray)\n\u001B[32m--> \u001B[39m\u001B[32m320\u001B[39m images_raw = images \u001B[38;5;28;01mif\u001B[39;00m is_tensor \u001B[38;5;28;01melse\u001B[39;00m [\u001B[43mload_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m img \u001B[38;5;129;01min\u001B[39;00m images]\n\u001B[32m    321\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m images_raw\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\aswin-github\\venv enviroment\\cv-keras\\Lib\\site-packages\\mtcnn\\utils\\images.py:296\u001B[39m, in \u001B[36mload_image\u001B[39m\u001B[34m(image, dtype, device)\u001B[39m\n\u001B[32m    293\u001B[39m         image_data = image  \u001B[38;5;66;03m# If file not found, use the input directly\u001B[39;00m\n\u001B[32m    295\u001B[39m     \u001B[38;5;66;03m# Decode the image with 3 channels (RGB)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m296\u001B[39m     decoded_image = \u001B[43mtf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdecode_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchannels\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m.numpy()\n\u001B[32m    298\u001B[39m \u001B[38;5;66;03m# If dtype is float, adjust the image scale\u001B[39;00m\n\u001B[32m    299\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m dtype \u001B[38;5;129;01min\u001B[39;00m [tf.float16, tf.float32]:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\aswin-github\\venv enviroment\\cv-keras\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    151\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    152\u001B[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001B[32m--> \u001B[39m\u001B[32m153\u001B[39m   \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    154\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    155\u001B[39m   \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\aswin-github\\venv enviroment\\cv-keras\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:108\u001B[39m, in \u001B[36mconvert_to_eager_tensor\u001B[39m\u001B[34m(value, ctx, dtype)\u001B[39m\n\u001B[32m    106\u001B[39m     dtype = dtypes.as_dtype(dtype).as_datatype_enum\n\u001B[32m    107\u001B[39m ctx.ensure_initialized()\n\u001B[32m--> \u001B[39m\u001B[32m108\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mops\u001B[49m\u001B[43m.\u001B[49m\u001B[43mEagerTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mValueError\u001B[39m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb6116e-d6af-49e7-969a-0c1ddea62b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
